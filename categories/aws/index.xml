<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AWS on Harisankar P S | HsPS.in</title>
    <link>https://hsps.in/categories/aws/</link>
    <description>Recent content in AWS on Harisankar P S | HsPS.in</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Mon, 23 Sep 2024 04:38:52 -0700</lastBuildDate><atom:link href="https://hsps.in/categories/aws/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Using Docker to export &amp; import data from Amazon RDS</title>
      <link>https://hsps.in/post/using-docker-postgres-to-take-sql-dump-from-aws-rds/</link>
      <pubDate>Mon, 23 Sep 2024 04:38:52 -0700</pubDate>
      
      <guid>https://hsps.in/post/using-docker-postgres-to-take-sql-dump-from-aws-rds/</guid>
      <description>&lt;p&gt;Docker is an efficient way to use command-line tools without needing to install them or their required libraries. Recently, I had a use case where I needed to take an SQL dump of a schema from a PostgreSQL database. The problem was that the PostgreSQL DB was version 16.3, while my local DB was only version 15. Using my local &lt;code&gt;pg_dump&lt;/code&gt; to access the remote database resulted in a version incompatibility error. To fix this, I would either need to upgrade &lt;code&gt;pg_dump&lt;/code&gt; or have both versions installed simultaneously on my machine. I didn&amp;rsquo;t want to do either. This is where Docker came to the rescue.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>AWS ECR Lifecycle Policy</title>
      <link>https://hsps.in/post/aws-ecr-lifecycle-policy/</link>
      <pubDate>Sat, 06 Apr 2024 06:21:43 -0700</pubDate>
      
      <guid>https://hsps.in/post/aws-ecr-lifecycle-policy/</guid>
      <description>&lt;p&gt;Amazon Elastic Container Registry (ECR) is a service provided by Amazon for hosting our Docker/container images. The cost of downloading these images to a server or ECS (Elastic Container Service) within the same zone is free. However, downloading or uploading from the public internet incurs costs, as does storage space usage.&lt;/p&gt;
&lt;p&gt;During the development and deployment of your project, you may find yourself building images as frequently as every 30 minutes. This frequency reflects my own experience. Whenever new code is merged with the master branch, it triggers a code build pipeline. This pipeline ultimately pushes the latest image to ECR, typically tagged as &amp;ldquo;latest.&amp;rdquo; Consequently, the previous image remains in the repository but becomes untagged. While retaining a few versions of older deployments facilitates easy rollbacks, after a day or two, they tend to become redundant. At this juncture, they serve no purpose other than incurring storage costs.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Reduce Cost of Aws Bill - Part 1</title>
      <link>https://hsps.in/post/reduce-cost-of-aws-bill-part-1/</link>
      <pubDate>Sun, 19 Mar 2023 19:52:42 -0700</pubDate>
      
      <guid>https://hsps.in/post/reduce-cost-of-aws-bill-part-1/</guid>
      <description>&lt;p&gt;Developing and deploying to the cloud has become a mandatory requirement than a nice to have skill. The promise of the cloud is that it abstracts all the complexity of managing a server and you only pay for what you use. If you accept that at face value, then the cloud is 100% legit. You only pay for what you use (just the cost is high), and they do abstract the regular complexity, and create some new complexity to worry about. There is a reason why AWS offers certificate programs. AWS Certified Cloud Practitioner/Architect/SysOps Admin/Developer/Solution Architect/Dev Ops Engineer all these shouldn&amp;rsquo;t be there if all the complexity were truly abstracted.&lt;/p&gt;
&lt;p&gt;Now that I have finished writing my frustration about cloud, lets return to the main topic of this article. Recently I was tasked with reducing our AWS bill. Our general approach to reduction in system performance have been to increase the server spec or DB spec. Looking at which resource was maxing out. DB or Server.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Adding New Key per User to AWS EC2 Instance</title>
      <link>https://hsps.in/post/adding-new-key-per-user-to-aws-ssh/</link>
      <pubDate>Fri, 01 Jul 2022 22:58:41 -0700</pubDate>
      
      <guid>https://hsps.in/post/adding-new-key-per-user-to-aws-ssh/</guid>
      <description>&lt;p&gt;When we create a new server in aws, it allows us to generate a key pair and attach it to the server. Now imagine you want to share access to this multiple people in your team, but you don&amp;rsquo;t want to share your private key. This is what you need to do.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Generate new key for each member of your team or ask each member for there public keys&lt;/li&gt;
&lt;li&gt;Add it to the authorized_keys list in your servers &lt;code&gt;.ssh&lt;/code&gt; folder&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
  </channel>
</rss>
