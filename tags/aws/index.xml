<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Aws on Harisankar P S | HsPS.in</title>
    <link>https://hsps.in/tags/aws/</link>
    <description>Recent content in Aws on Harisankar P S | HsPS.in</description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Thu, 10 Oct 2024 00:05:58 -0700</lastBuildDate>
    <atom:link href="https://hsps.in/tags/aws/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Running Rails Migrations in AWS ECS Using AWS CLI</title>
      <link>https://hsps.in/post/run-rails-db-migrate-on-ecs-task-aws/</link>
      <pubDate>Thu, 10 Oct 2024 00:05:58 -0700</pubDate>
      <guid>https://hsps.in/post/run-rails-db-migrate-on-ecs-task-aws/</guid>
      <description>&lt;p&gt;AWS ECS (Elastic Container Service) is a powerful Amazon Web Service that allows you to deploy Docker containers on various infrastructure options, including EC2 instances, Fargate (serverless containers), or even self-hosted servers. Being deeply integrated with AWS, ECS offers a cost-effective and seamless solution for managing containers in the Amazon ecosystem.&lt;/p&gt;&#xA;&lt;p&gt;This guide assumes you are familiar with ECS and already have a task definition set up for your Rails application. Additionally, it assumes you know how to find your subnet ID and security group ID, as these will be required when running containers in your specific environment.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Using Docker to export &amp; import data from Amazon RDS</title>
      <link>https://hsps.in/post/using-docker-postgres-to-take-sql-dump-from-aws-rds/</link>
      <pubDate>Mon, 23 Sep 2024 04:38:52 -0700</pubDate>
      <guid>https://hsps.in/post/using-docker-postgres-to-take-sql-dump-from-aws-rds/</guid>
      <description>&lt;p&gt;Docker is an efficient way to use command-line tools without needing to install them or their required libraries. Recently, I had a use case where I needed to take an SQL dump of a schema from a PostgreSQL database. The problem was that the PostgreSQL DB was version 16.3, while my local DB was only version 15. Using my local &lt;code&gt;pg_dump&lt;/code&gt; to access the remote database resulted in a version incompatibility error. To fix this, I would either need to upgrade &lt;code&gt;pg_dump&lt;/code&gt; or have both versions installed simultaneously on my machine. I didn&amp;rsquo;t want to do either. This is where Docker came to the rescue.&lt;/p&gt;</description>
    </item>
    <item>
      <title>AWS ECR Lifecycle Policy</title>
      <link>https://hsps.in/post/aws-ecr-lifecycle-policy/</link>
      <pubDate>Sat, 06 Apr 2024 06:21:43 -0700</pubDate>
      <guid>https://hsps.in/post/aws-ecr-lifecycle-policy/</guid>
      <description>&lt;p&gt;Amazon Elastic Container Registry (ECR) is a service provided by Amazon for hosting our Docker/container images. The cost of downloading these images to a server or ECS (Elastic Container Service) within the same zone is free. However, downloading or uploading from the public internet incurs costs, as does storage space usage.&lt;/p&gt;&#xA;&lt;p&gt;During the development and deployment of your project, you may find yourself building images as frequently as every 30 minutes. This frequency reflects my own experience. Whenever new code is merged with the master branch, it triggers a code build pipeline. This pipeline ultimately pushes the latest image to ECR, typically tagged as &amp;ldquo;latest.&amp;rdquo; Consequently, the previous image remains in the repository but becomes untagged. While retaining a few versions of older deployments facilitates easy rollbacks, after a day or two, they tend to become redundant. At this juncture, they serve no purpose other than incurring storage costs.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Sidekiq Graceful Shutdown in Fargate ECS</title>
      <link>https://hsps.in/post/sidekiq-graceful-shutdown-in-fargate-ecs/</link>
      <pubDate>Sun, 30 Jul 2023 00:10:21 -0700</pubDate>
      <guid>https://hsps.in/post/sidekiq-graceful-shutdown-in-fargate-ecs/</guid>
      <description>&lt;p&gt;Sidekiq is the gold standard when it comes to background processing. I have been using since I started working with rails. Sidekiq took care of lot of the complexity of the background processing that allowed developers to concentrate on just there code/business logic.&lt;/p&gt;&#xA;&lt;p&gt;One of the beautiful features of sidekiq was how it handled exit (SIGTERM and SIGKILL).&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;https://turnoff.us/image/en/dont-sigkill-2.png&#34; alt=&#34;Dont Sigkill&#34;&gt;&#xA;Source: &lt;a href=&#34;https://turnoff.us/geek/dont-sigkill-2/&#34;&gt;https://turnoff.us/geek/dont-sigkill-2/&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;When sidekiq received the sigterm commands it stops accept new jobs, and just focus on finishing the job it is doing now and incase it can&amp;rsquo;t finish, it will requeue the job to another available worker or keep it in the queue for another worker (or the same worker) to pick it up when available.&#xA;This was working so well for years on rails app I was working on, that I didn&amp;rsquo;t even care about it. The mina command would trigger a sidekiq restart allowing sidekiq to gracefully exit and then respawn with the new code.&lt;/p&gt;&#xA;&lt;p&gt;The above app was running on two linux servers everything manually configured, but recently to keep up with the growth of the app we moved to amazon ECS. The app was containerized to a docker image, and made to run on about 12 Fargate Instances with ability to auto scale to 36 based on CPU and RAM usage. We grouped instances for various purposes 2 for heavy jobs, 2 for faster (more memory jobs), 2 for file processing, 4 for web, 2 for API.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Reduce Cost of Aws Bill - Part 1</title>
      <link>https://hsps.in/post/reduce-cost-of-aws-bill-part-1/</link>
      <pubDate>Sun, 19 Mar 2023 19:52:42 -0700</pubDate>
      <guid>https://hsps.in/post/reduce-cost-of-aws-bill-part-1/</guid>
      <description>&lt;p&gt;Developing and deploying to the cloud has become a mandatory requirement than a nice to have skill. The promise of the cloud is that it abstracts all the complexity of managing a server and you only pay for what you use. If you accept that at face value, then the cloud is 100% legit. You only pay for what you use (just the cost is high), and they do abstract the regular complexity, and create some new complexity to worry about. There is a reason why AWS offers certificate programs. AWS Certified Cloud Practitioner/Architect/SysOps Admin/Developer/Solution Architect/Dev Ops Engineer all these shouldn&amp;rsquo;t be there if all the complexity were truly abstracted.&lt;/p&gt;&#xA;&lt;p&gt;Now that I have finished writing my frustration about cloud, lets return to the main topic of this article. Recently I was tasked with reducing our AWS bill. Our general approach to reduction in system performance have been to increase the server spec or DB spec. Looking at which resource was maxing out. DB or Server.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Adding New Key per User to AWS EC2 Instance</title>
      <link>https://hsps.in/post/adding-new-key-per-user-to-aws-ssh/</link>
      <pubDate>Fri, 01 Jul 2022 22:58:41 -0700</pubDate>
      <guid>https://hsps.in/post/adding-new-key-per-user-to-aws-ssh/</guid>
      <description>&lt;p&gt;When we create a new server in aws, it allows us to generate a key pair and attach it to the server. Now imagine you want to share access to this multiple people in your team, but you don&amp;rsquo;t want to share your private key. This is what you need to do.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Generate new key for each member of your team or ask each member for there public keys&lt;/li&gt;&#xA;&lt;li&gt;Add it to the authorized_keys list in your servers &lt;code&gt;.ssh&lt;/code&gt; folder&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
  </channel>
</rss>
