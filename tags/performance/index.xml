<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Performance on Harisankar P S | Ruby on Rails Developer</title>
    <link>https://hsps.in/tags/performance/</link>
    <description>Recent content in Performance on Harisankar P S | Ruby on Rails Developer</description>
    <generator>Hugo</generator>
    <language>en</language>
    <copyright>Harisankar P S</copyright>
    <lastBuildDate>Sat, 26 Jul 2025 21:47:37 +0530</lastBuildDate>
    <atom:link href="https://hsps.in/tags/performance/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Caching Rendered PDFs in Rails with Active Storage</title>
      <link>https://hsps.in/post/using-file-upload-to-cache-serving-pdf-files/</link>
      <pubDate>Sat, 26 Jul 2025 21:47:37 +0530</pubDate>
      <guid>https://hsps.in/post/using-file-upload-to-cache-serving-pdf-files/</guid>
      <description>&lt;p&gt;As I was working on &lt;a href=&#34;https://easyclientlog.com&#34;&gt;easyclientlog.com&lt;/a&gt;, building its invoice system that allows freelancers/consultants to generates PDFs. Working with PDF generating is that, its takes time and CPU cycles. But most of the time&#xA;the pdf only needs to be generated once, and the content seldom changes. So rendering the same thing over and over just didn’t feel right. So I used a simple trick that I’ve followed in many of my prior Rails apps: upload the PDF on first render, store it using Active Storage, and reuse that file on the next access.&lt;/p&gt;&#xA;&lt;h2 id=&#34;the-idea-cache-on-first-render&#34;&gt;The Idea: Cache on First Render&lt;/h2&gt;&#xA;&lt;p&gt;When the PDF is generated the first time, we attach it to the record using Active Storage. This could be an invoice, report, or any other object. The next time we need to show or download the PDF, we skip the rendering step and simply serve the uploaded file.&lt;/p&gt;&#xA;&lt;p&gt;This avoids unnecessary rendering and speeds up response times, especially for large documents or when generating in bulk.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Storing PostgreSQL Data in a Different Partition for Performance</title>
      <link>https://hsps.in/post/storing-postgresql-data-in-a-different-partition-for-performance/</link>
      <pubDate>Wed, 15 Jan 2025 18:20:19 +0530</pubDate>
      <guid>https://hsps.in/post/storing-postgresql-data-in-a-different-partition-for-performance/</guid>
      <description>&lt;p&gt;What I am suggesting is a method of optimization not just for PostgreSQL but for other similar databases as well. This is a more primitive, hardware-level performance optimization where an exclusive partition or hard disk is dedicated just for the actual data. By doing so, all the read and write operations related to your data happen on that disk, while OS-level and software-level read/write operations happen on another disk. This approach also makes it easier to handle failure, monitor disk health, and isolate workloads effectively.&lt;/p&gt;&#xA;&lt;h2 id=&#34;why-store-postgresql-data-in-a-separate-partition&#34;&gt;Why Store PostgreSQL Data in a Separate Partition?&lt;/h2&gt;&#xA;&lt;p&gt;Separating the data directory from the root filesystem has several advantages:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;strong&gt;Improved Disk I/O Performance:&lt;/strong&gt; Storing data on a dedicated partition ensures that database read/write operations are isolated from the OS and application processes, avoiding competition for disk I/O.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Efficient Disk Usage Management:&lt;/strong&gt; By placing the data in a separate partition, you can allocate specific disk space for the database and prevent it from filling up the root partition.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Better Backup and Restore Control:&lt;/strong&gt; Storing data in a dedicated location simplifies backup processes and makes restoring data more efficient.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Optimized Disk Types:&lt;/strong&gt; Different types of storage media (e.g., SSD for faster reads/writes or HDD for archival data) can be used based on database needs.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Isolation for Security and Resilience:&lt;/strong&gt; A dedicated partition reduces the risk of system failure in case of database corruption or storage issues.&lt;/li&gt;&#xA;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Setup Read Replica (master-slave) Database on Postgresql 17</title>
      <link>https://hsps.in/post/setup-master-slave-database-on-postgresql-17/</link>
      <pubDate>Thu, 24 Oct 2024 10:19:48 -0700</pubDate>
      <guid>https://hsps.in/post/setup-master-slave-database-on-postgresql-17/</guid>
      <description>&lt;p&gt;PostgreSQL is a mature, open-source relational database that has been widely adopted by many companies over the years. It has been my go-to database for most of my software career, helping me solve various performance challenges in my projects. Features like JSON generation, materialized views, and others have enabled me to scale APIs to handle millions of requests per second.&lt;/p&gt;&#xA;&lt;p&gt;One effective strategy to improve the performance of web applications is to create a read-only replica of the primary database. This allows you to direct read-only queries (e.g., &lt;code&gt;SELECT&lt;/code&gt; queries) to the replica, while write operations continue to target the primary database. Note that PostgreSQL doesn’t support multiple primary (or master) databases natively, but it does support multiple read replicas.&lt;/p&gt;&#xA;&lt;p&gt;Although you can achieve distributed primary databases via schema or table sharding, that topic is beyond the scope of this article.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
